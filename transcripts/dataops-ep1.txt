Welcome to Data Ops. Podcast. I'm your host, Banjo BME. And I'm Victoria Guido. It's a banjo. Tell me a little bit about yourself. Yeah. So I, um, a software engineer at 26 Labs. I've been doing computer science almost 10 years now. I graduated from University of Maryland and the great computer science graduate to be a University of Loyola in computer science. This always loved doting of natural doder. I've always been making my own APS. I have deployed absolutely app store. I've done a lot of cool gadgets and gizmos like Alexa APS have done that. That's pretty, uh, also love going to meetups. I started my own media card data ops, DC to kind of help bridge the gap between data scientist, devil practitioner. That kind of bring of a new age of my building data gets that one the way we met at Dev Ops D C, right? Yeah. I'm the vice president of operations at a small federal consulting company called STS. I. I worked in I t for the last 10 to 12 years. I started off with a degree in economics from University of Maryland and then went back and got my master's in I T. And Project Management. So someone described me recently as a person who gets text things done, which is why I really like to learn about what people are doing, how they're making decisions. And my speciality, I think, is involving that strategy. And how are we gonna accomplish something really complicated and difficult and sharing that information with as many people as I possibly get its mother? Yeah, that's why I read that the double do you see meet up and I asked, I'm a director with women, you code. So I helped start up our Deb ups serious as well as support the data group and the python of Lee's getting their meet ups up and running and, you know, supporting them with their journey and bringing more women into the data signs world. So it's awesome. That's awesome. So why don't we explain why we're starting this code cast? But reason for starting this podcast is as a Dev ops fan. I want to know more about the practice of data science and how we should consider data when we're building products and systems which really center around data and data science and it's been at the software engineer. I've been doing a lot of cool projects involving De Bob, and big Data become a big thing now. And it's not really any process how to manage all this data and use like a dead possum mindset, to kind of bring all the data and build abs and building analytics and all sorts of things. Your data and I think Daytop kind of encapsulates everything needed to get value from your data, right? So what is data? APS? That's a loaded term Data is pretty much how I like to describe it, creating a process to create value from your data. And there's so many things that involved in that from identifying where your data is, collecting the data, operationalize ing what you're doing and then telling that story of how you created value. All right, and there's this thing out there called the Data Ops manifesto Great, and it seems very similar to the agile manifesto and a lot of similar concepts. But I think that there's a few particular ideas which are really specific to data that he get into a little bit more. So one item is you know, we talked about data pipelines a lot. So what's it? What's data pipeline? Any pipeline is basically creek accessing your data and then maybe have you heard of e t l extract? Transform load. You want to be ableto get your data and move it to another location may be performed. Analytics for information set. And I'm putting in another location will give it access to somebody. So having controlling the data every step of that pipeline data pipeline essential for creating data and making sure you get value from that. Yeah, absolutely. And I think there's some really interesting concepts from there, which we wanted to get into you throughout this series. Analytics as code. Ensuring you have reproducible results is a really important aspect. And one of the other reasons we wanted to start this is because with things like the federal data strategy that's come out with all of this interest in data science and how we can create value from data, there's a need to expand on our knowledge and awareness of how do you manage that data? How do you make sure that you're using the same principles that you would do for any kind of software development into a data strategy where you might have, like, millions of lines of data. I mean, neighborhood has been exploding the last couple of years, and so many agencies have lots of data, but only, you know, effectively know how to utilize. Didn't quick value from that. You hear a lot about machine learning and AI coming though the all power through data. So having a very stolid data pipeline and data ops platform to help accelerate that growth is gonna be key. Yeah, absolutely. And we can't even get to an example right now about what kind of problem agency might want to solve with data and how we can kind of address that problem moving forward. So, one example is the Library of Congress put out, or if I a couple of weeks ago or the response to it was a couple weeks ago to build the next generation library system. Eso. If you don't know about the library Congress, it's the largest library in the world. They have over 158 million records, and those records include books and bibliography, information from millions of books and 450 different languages. They've got Manu scripts. They've got video archives, they've got music, music, sheets and that's a lot of data. And that's their adding to its 600 to 400,000 records every single year. Right? And I have different types of data, and the file types will be enormous on day. Want to know, Like, how can we better architect our data energy systems to get more value out of this, to make it easier to discover what did it on? If we want Teoh build any kind of algorithms that help the public be able to find what they want to find easier and even curate some of their collections for the rest of the library, so have many different end users to eternal youth with people. So how do you act as all the data control? Yeah, is they interface with libraries all over the world s Oh, there's all kinds of concerns there. So what approach? That we've got it going back and forth with for their problem here. So they've got two different systems that currently stored data, right? Millions of records, hundreds of millions of records in both. How do you go about creating a data architecture where you could start to build these data pipelines and be able to introduce them. Analytics is code as well. So tell me more what you think about creating a ET lto a data warehouse solution for something like library. Congress is the only one being with a module eyes smallest unit. So they have all these different type of books or record. You know, you have to have, like, a abstract away. Like I have a data unit from there. How can I collect that data? You know, how can I perform analytics on How can I store that you want to be thinking of, like how my house data flowing to my system. Where is it coming from? Who has access to it? Whose attic? It. So you want to kind of put pulling that data unit and putting metadata around that would be able to track that build applications from your data right. And you may not necessarily change the data that's in your original database, but start to extract that data and transform and loaded into a new data warehouse where you could manipulate it where you can add to it where you can start adding other sources into a singular data warehouse were. Now, you can kind of break that apart and build out micro services. You could build a new application, right? Having that single source of truth allows you to build intimately and kind of the spill from that this take away from that Data lake, if you will able the Building analytics code AP I so having that kind of whole process, the find is very important because we found having a singular data you need not be able to build. Yeah, like now you have your data warehouse built, right? You've got this, like, huge repository with all these different data things in it. What do you think you would start if you're a library? Congress? And you wanted to put some cool data analytics and really insights from all this data that you have. Where would you think you would start? Well, I would always ask the customer What is it important to them? What's gonna give them value? You know, you want to identify what they're trying to do? Why is it important to them? Why do they need access to this data? And how is it going to help the mission. So kind of talking. Those conversations burst and voting upon that because people want different things. But it's hard at birth, and I think having a good, solid gold is so to so something to them. And then when they say, Oh, I don't like it like this, it helps to build that story's gonna start somewhere, identify, then start. Yeah, and maybe we can assume from the art by. One of the goals that library Congress has is to make their data more discoverable. So to make it easier to find what you're looking for, to make the searching easier from the public so that they can access the materials that they're looking for, what types of DEA science tools you think you would start to suggest for that type of thing? Yeah, talking about metadata. Guess when we can access the data and then maybe you have some kind of repository. Depend how those strong data like you make a P. I think that's kind of lead from them. That's delivered to a person simple format, so could be digested by other people. So, Jason, former, very universal everywhere. It's just easier to store. What, someone you think is also told like crap. Well, which is a new kind of query language. So it's one endpoint. A person can query the endpoint Senate question and get data back however they want. So I think grab to a good standard to go for something as massive is. It was so much data that people were on specific things. Grab card. A very good way to kind of have a defined data language and then be able to quit one endpoint and get what you need. Exactly. Ok, uh, do you think, like, linked open data? How does that kind of play into all of this as well? To make things more discoverable. Yeah. So when you have called my different customer that different partners having that kind of one end 10.1 standard in the graft diplomat allowed everybody to write to that one speck and wait to get the same data what was using it? So I think having a universal standard and graph to have been going a lot of popularity in the past couple of years, I get have a nice graph Joy, the island piece of starting to develop it and you delighted. Kind of a placing Rest rest will come on the order first. Like here. You get something, get all the data. One of the big problems that you get. Everything you know, you don't need every It's hard toe granule I Some of these search queries. So graft kind of helping solve that problem. I think a lot of these bigger open access systems start using Cool. You're going to a graphic. You'll conference this week, right? Yes. I'm actually speaking at Baikonur, which is up online flea conference. I'm giving a talk on How do you get up? Grab Kill eight guys. So you're gonna, like, use graphic. You all to go in final. This data from getting a job and do analysis on the code is out there. Right? So get have a trove of data from comments. Pull requests like Kodi was poured languages. So much stuff. How do you, like make a mining platform to grab all that and stored in my explore meant that we utilize for other applications later. Cool. What kind of questions? Their problems. Are you solving with that? Kind of with that approach, then. So like like how many people are right to a specific repository. How money comments are like in a public West kind of getting meta data and a large corporate get project. I want to large scale analysis of, like hundreds of thousands product confined and find friends like how people develop, how coders are coded and pushing over time Except you getting kind of like human analysis, how people develop code to get up. Okay, that's really cool. So that's Saturday, Friday, Friday, And then on Saturday there's a python to end of life party, right? That's correct. Yeah, I'll be a panel if they're talking about data science in the future like type on three in the developing machine. Learning What is the future of Python three? Everybody wanted to know a pipelines, grains, so much popularity in the past couple years. And I think data scientists really helped force that. You know, a lot of cool library people use like that scaler on attempted flow pipe towards and really lowered the bar to get people interested in machine money, I think, was driving a lot of new development into these libraries and pipe bombs of the great entry language. It's easy to learn mixes a lot of different other things. I think it's getting more popular. Yeah, absolutely. Even my boss uses Python to scrape the Internet for data regarding to sports. Betting is a 1,000,000 uses out there. Any problem you could think of Beacon? Figure out a way to get some data about it and see what information you can come up with. Absolutely. Let's see, What else should we talk? How about that? We want to get people on. We want to bring them on the show and just understand what problems were they trying to stop, what decisions they make regarding the tools they were using and what were the trade offs to what they were trying to do. So maybe a python works for one reason, but it has certain limitations in a type of analysis, and you need to expand into something else or if you need to collaborate across the team, how does that change what kind of tools you start using? That's what I would like to learn more about right, how people solving data problem and tools or using other things are encountering this to get inside, like how this field is evolving. Yeah, it is. It's a new pin, Newfield. I know. When I got my degree in economics and I wanted to go in, I t There is no such thing as day signs. Now, if I would be able to go back, I might. You decided to go with the data size route, but here we are. That's cool. Yeah, Well, ended here and tune in for the next data ups podcast. Thank you, everybody
